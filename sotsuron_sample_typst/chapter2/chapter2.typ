= 関連研究

== 概要
本章では、 動的環境下におけるルーティングアルゴリズムや強化学習についての先行研究について触れ、本研究の目的及び位置付けを明確にする。

//それぞれの研究に今回の研究との関連を。2.3はいらないかも。

== 先行研究
=== Q-routing: 動的ネットワーク環境における経路探索アルゴリズム
//(J A.Boyan)
Q-routingはJustin A.Boyan並びMichael L.Littmanらが提唱したアルゴリズムである@boyan1993packet 。この手法は動的なネットワーク環境における最適な経路探索を目的とした分散型アルゴリズムであり、強化学習の1つであるQ学習をベースにしている。

Q-routingでは、ネットワーク内の各ノードをエージェントとし、そのノード間の通信を通じ逐次的に学習を行うことにより動的に変化するネットワーク環境に適応することを目的としている。この手法では各ノードが隣接ノードへの遅延を予測し、その情報をもとに状態-行動の評価値(これをQ値とする)を更新する仕組みをもつ。このため、変化のある環境においても効率的に最適経路を学習できる特性を有している。

さらに、この手法では複数の目標ノードに向けた学習を並行して行うことが可能である。各ノードが他ノードと直接的な予測を共有することはできないが、隣接ノードから得られる遅延情報を利用することでネットワーク全体の動的環境における効率的なパケットルーティングを実現することができる。


=== 報酬が周期的に変化する環境のための強化学習
//(澁谷 長史, 安信 誠二)
澁谷長史及び安信 誠二らによる論文「報酬が周期的に変化する環境のための強化学習」@澁谷長史2014 に提案されている強化学習の手法は、報酬が時間によって周期的に変化する環境での効率的な学習を目的としている。この手法では従来の強化学習アルゴリズムの持つ収束の遅さや試行回数の増加という課題を解決するため提案された。従来の強化学習では固定的な報酬を前提とすることが多いが、この手法において時間依存の報酬を考慮し周期的な環境変化に適応可能なアルゴリズムを提案している。

この手法では時刻要素を複素数平面上のフェーザ形式で表現し、エージェントが望ましい行動を決定するため過去の報酬から評価を算出する行動価値関数にそのフェーザ表示を組み込むことにより、状態空間の膨張を抑えながら動的変化に対応できる特性を有している。実験結果では、報酬が時間に応じて明確な周期性を伴う場合、従来のアルゴリズム(Sarsaアルゴリズム)と比較し学習の収束速度が向上することが確認されている。一方で報酬の周期性がフェーザ形式で表現が難しく曖昧な周期の場合にはこの手法を扱うのが困難であるという課題がある。

== 先行研究との比較及び本研究の位置づけ
本研究では、複数の周期的変化が混在する環境において、各周期の最も快適な経路を選択する深層強化学習モデルを提案する。従来の動的環境対応アルゴリズムであるQ-routing@boyan1993packet は、リアルタイムでのネットワーク変化に対応可能であるが、周期的な変化を考慮しておらず、特に短周期や長周期が同時に存在する状況では適応が難しい。一方、報酬の周期性に特化した澁谷氏らの手法@澁谷長史2014 は、周期的変化への効率的な対応を可能にしたが、非周期的な変化や複数の周期的要因が混在する環境への適用には限界がある。

その課題をもとに、本研究ではカウンター値を用いて環境の周期性を管理し、これを深層強化学習モデルの入力として取り込む設計を採用することで、複数の周期を持つ環境に適応可能な経路選択を実現し、その性能を従来手法と比較することで評価する。

#pagebreak()

/*この手法はノード間の通信を通じ、逐次的な学習を行うことにより、動的に変化が進んでいくネットワーク状態に適応することを目的としている。この手法ではネットワークを構成するノードそれぞれをエージェントとしたアルゴリズムであり、目標ノードが異なる複数のグループを環境内に置くことが可能となっている。このグループはそれぞれのグループで相互に予測することは難しいが、複数の方策の学習を取り入れながら他の方策の学習へ反映できるため、動的な環境においても学習を利用できる特性を持つ。

//
Justin A. BoyanおよびMichael L. Littmanらが提唱したアルゴリズムに「Q-routing」がある【@boyan1993packet】。この手法は、動的なネットワーク環境における最適な経路探索を目的とした分散型アルゴリズムであり、強化学習の一種であるQ学習を基盤としている。

Q-routingは、ネットワーク内の各ノードをエージェントと見なし、ノード間の通信を通じて逐次的に学習を行うことで、動的に変化するネットワーク状態に適応することを目指している。この手法では、各ノードが隣接ノードへの通信遅延を観測し、その情報を基に状態-行動ペアの評価値（Q値）を更新する仕組みを持つ。これにより、リアルタイムで変化する環境でも、分散的かつ効率的に最適経路を学習できる特性を有する。

さらに、Q-routingでは複数の宛先ノード（目標ノード）に向けたルート情報を並行して学習することが可能である。各ノードが他のノードと直接的な予測を共有することはないが、隣接ノードから得られる遅延情報を利用し、ネットワーク全体として動的環境における効率的なパケットルーティングを実現する。この分散型アルゴリズムの特性は、中央制御が不要でスケーラビリティが高く、大規模ネットワークやリアルタイム処理が求められる環境において有効である。
*/