= 結言

== 結論
本研究では、周期的に変化する環境に適応するための深層強化学習の手法を提案し、その有効性を検証した。従来のDeep Q-networkモデルを基礎にして、カウンター値や周期情報を入力要素に組み込むことで、周期的環境下での適応性を向上させることが確認できた。

また、本研究で行った実験の結果から、カウンター値を正規化して入力する手法が最も安定した学習性能を示すことが分かったが、環境によって他2手法の安定性や適応力に考慮の余地があることが示された。モジュロ演算の入力は短期的な環境変化に反応する挙動が見られたが、長期的な安定性が課題として残った。インデックス指定値の入力は、各周期の変化に反応を示し期待する経路に対する順応性を見せたが、確定性に欠ける挙動が課題として挙げられる。

== 今後の展望
本研究の手法では、時間要素を入力することで動的環境における学習の有用性を示した。しかし、現実の環境においては、時間以外にも様々な要素が経路選択や行動決定に関与する。例えば、距離や混雑度、移動コスト、さらには安全性や利便性といった要素が複雑に絡み合い、最適な経路を選択するための判断材料となる。このような要素を考慮することで、より現実に近い環境モデルを構築できる可能性がある。

特に、経路と移動時間のトレードオフを考慮した環境を設計することで、単なる時間短縮だけでなく、「快適な経路」という観点からさらなる精度向上が期待される。このような環境では、例えば混雑を避けたルートや距離が短い代わりに安全性が高いルートを学習する能力が求められる。これにより、現実的な応用において、提案手法がより多様な条件下で柔軟に対応できるようになると考えられる。

/*== 総括
両実験において、提案手法のうち、カウンター値の正規化入力が優れた適応力を示す結果となった。一方、環境によって他2手法の安定性や適応力にも考慮の余地があるとわかった。*/